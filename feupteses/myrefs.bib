%bibtex entries

@article{7waystodie,
  author={El Emam, Khaled},
  journal={IEEE Security \& Privacy}, 
  title={Seven Ways to Evaluate the Utility of Synthetic Data}, 
  year={2020},
  volume={18},
  number={4},
  pages={56-59},
  doi={10.1109/MSEC.2020.2992821}}


@article{reif2014automatic,
	author = {Reif, Matthias and Shafait, Faisal and Goldstein, Markus and Breuel, Thomas and Dengel, Andreas},
	da = {2014/02/01},
	date-added = {2022-02-28 17:55:51 +0000},
	date-modified = {2022-02-28 17:55:51 +0000},
	doi = {10.1007/s10044-012-0280-z},
	id = {Reif2014},
	isbn = {1433-755X},
	journal = {Pattern Analysis and Applications},
	number = {1},
	pages = {83--96},
	title = {{Automatic Classifier Selection for Non-Experts}},
	ty = {JOUR},
	url = {https://doi.org/10.1007/s10044-012-0280-z},
	volume = {17},
	year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1007/s10044-012-0280-z}}

@article{Arvanitis2021.02.11.21250741,
	title        = {{A Method for Machine Learning Generation of Realistic Synthetic Datasets for Validating Healthcare Applications}},
	author       = {Arvanitis, Theodoros N. and White, Sean and Harrison, Stuart and Chaplin, Rupert and Despotou, George},
	year         = 2021,
	journal      = {medRxiv},
	publisher    = {Cold Spring Harbor Laboratory Press},
	doi          = {10.1101/2021.02.11.21250741},
	url          = {https://www.medrxiv.org/content/early/2021/02/12/2021.02.11.21250741},
	elocation-id = {2021.02.11.21250741},
	eprint       = {https://www.medrxiv.org/content/early/2021/02/12/2021.02.11.21250741.full.pdf},
}
@article{rivolli2019characterizing,
	doi = {10.48550/ARXIV.1808.10406},
  url = {https://arxiv.org/abs/1808.10406},
  author = {Rivolli, Adriano and Garcia, Luís P. F. and Soares, Carlos and Vanschoren, Joaquin and de Carvalho, André C. P. L. F.},
  journal={arXiv: Learning},
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {{Characterizing Classification Datasets: A Study of Meta-Features for Meta-Learning}},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@inproceedings{10.1145/342009.335437,
	title        = {Efficient Algorithms for Mining Outliers from Large Data Sets},
	author       = {Ramaswamy, Sridhar and Rastogi, Rajeev and Shim, Kyuseok},
	year         = 2000,
	booktitle    = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
	location     = {Dallas, Texas, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {SIGMOD '00},
	pages        = {427–438},
	doi          = {10.1145/342009.335437},
	isbn         = 1581132174,
	url          = {https://doi.org/10.1145/342009.335437},
	abstract     = {In this paper, we propose a novel formulation for distance-based outliers that is based on the distance of a point from its kth nearest neighbor. We rank each point on the basis of its distance to its kth nearest neighbor and declare the top n points in this ranking to be outliers. In addition to developing relatively straightforward solutions to finding such outliers based on the classical nested-loop join and index join algorithms, we develop a highly efficient partition-based algorithm for mining outliers. This algorithm first partitions the input data set into disjoint subsets, and then prunes entire partitions as soon as it is determined that they cannot contain outliers. This results in substantial savings in computation. We present the results of an extensive experimental study on real-life and synthetic data sets. The results from a real-life NBA database highlight and reveal several expected and unexpected aspects of the database. The results from a study on synthetic data sets demonstrate that the partition-based algorithm scales well with respect to both data set size and data set dimensionality.},
	numpages     = 12
}
@article{10.1145/3158346,
author = {Sethi, Tegjyot Singh and Kantardzic, Mehmed},
title = {When Good Machine Learning Leads to Bad Security: Big Data (Ubiquity Symposium)},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2018},
number = {May},
url = {https://doi.org/10.1145/3158346},
doi = {10.1145/3158346},
abstract = {While machine learning has proven to be promising in several application domains, our understanding of its behavior and limitations is still in its nascent stages. One such domain is that of cybersecurity, where machine learning models are replacing traditional rule based systems, owing to their ability to generalize and deal with large scale attacks which are not seen before. However, the naive transfer of machine learning principles to the domain of security needs to be taken with caution. Machine learning was not designed with security in mind and as such is prone to adversarial manipulation and reverse engineering. While most data based learning models rely on a static assumption of the world, the security landscape is one that is especially dynamic, with an ongoing never ending arms race between the system designer and the attackers. Any solution designed for such a domain needs to take into account an active adversary and needs to evolve over time, in the face of emerging threats. We term this as the "Dynamic Adversarial Mining" problem, and this paper provides motivation and foundation for this new interdisciplinary area of research, at the crossroads of machine learning, cybersecurity, and streaming data mining.},
journal = {Ubiquity},
month = {may},
articleno = {1},
numpages = {14}
}
@inproceedings{Kar_2019_ICCV,
	doi = {10.48550/ARXIV.1904.11621},
  url = {https://arxiv.org/abs/1904.11621},
  author = {Kar, Amlan and Prakash, Aayush and Liu, Ming-Yu and Cameracci, Eric and Yuan, Justin and Rusiniak, Matt and Acuna, David and Torralba, Antonio and Fidler, Sanja},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Graphics (cs.GR), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {{Meta-Sim: Learning to Generate Synthetic Datasets}},
  publisher = {arXiv},
  year = {2019},
	isbn = {978-1-7281-4803-8},
	issn            = {2380-7504},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@article{s19051181,
	title        = {{SynSys: A Synthetic Data Generation System for Healthcare Applications}},
	author       = {Dahmen, Jessamyn and Cook, Diane},
	year         = 2019,
	journal      = {Sensors},
	volume       = 19,
	number       = 5,
	doi          = {10.3390/s19051181},
	issn         = {1424-8220},
	url          = {https://www.mdpi.com/1424-8220/19/5/1181},
	article-number = 1181,
}
@incollection{binder2015analysis,
	title        = {{Analysis of Large-Scale OMIC Data Using Self Organizing Maps}},
	author       = {Binder, Hans and Wirth, Henry},
	year         = 2015,
	booktitle    = {Encyclopedia of Information Science and Technology, Third Edition},
	publisher    = {IGI global},
	pages        = {1642--1653}
}
@article{reif2012dataset,
	title        = {{Dataset Generation for Meta-Learning}},
	author       = {Reif, Matthias and Shafait, Faisal and Dengel, Andreas},
	year         = 2012,
	journal      = {KI-2012: Poster and Demo Track},
	publisher    = {Citeseer},
	doi             = {10.1007/978-3-030-33607-3_43},
	pages        = {69--73}
}
@article{DBLP:journals/corr/WangSSZXZ14,
	title        = {{A Feature Subset Selection Algorithm Automatic Recommendation Method}},
	author       = {Guangtao Wang and Qinbao Song and Heli Sun and Xueying Zhang and Baowen Xu and Yuming Zhou},
	year         = 2014,
	journal      = {Journal of Artificial Intelligence Research},
	volume       = {abs/1402.0570},
	url          = {http://arxiv.org/abs/1402.0570},
	eprinttype   = {arXiv},
	eprint       = {1402.0570},
	timestamp    = {Mon, 13 Aug 2018 16:47:11 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/WangSSZXZ14.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org},
	doi             = {https://doi.org/10.1613/jair.3831},
}
@inproceedings{10.1007/11526018_45,
	author = {Castiello, Ciro and Castellano, Giovanna and Fanelli, Anna Maria},
	title = {{Meta-Data: Characterization of Input Features for Meta-Learning}},
	year = {2005},
	isbn = {3540278710},
	publisher = {Springer-Verlag},
	address = {Berlin, Heidelberg},
	url = {https://doi.org/10.1007/11526018_45},
	doi = {10.1007/11526018_45},
	abstract = {Common inductive learning strategies offer the tools for knowledge acquisition, but possess some inherent limitations due to the use of fixed bias during the learning process. To overcome limitations of such base-learning approaches, a novel research trend is oriented to explore the potentialities of meta-learning, which is oriented to the development of mechanisms based on a dynamical search of bias. This could lead to an improvement of the base-learner performance on specific learning tasks, by profiting of the accumulated past experience. As a significant set of I/O data is needed for efficient base-learning, appropriate meta-data characterization is of crucial importance for useful meta-learning. In order to characterize meta-data, firstly a collection of meta-features discriminating among different base-level tasks should be identified. This paper focuses on the characterization of meta-data, through an analysis of meta-features that can capture the properties of specific tasks to be solved at base level. This kind of approach represents a first step toward the development of a meta-learning system, capable of suggesting the proper bias for base-learning different specific task domains.},
	booktitle = {Proceedings of the Second International Conference on Modeling Decisions for Artificial Intelligence},
	pages = {457–468},
	numpages = {12},
	location = {Tsukuba, Japan},
	series = {International Conference on Modeling Decisions for Artificial Intelligence'05}
}
@inproceedings{7382962,
	title        = {{Datasets Meta-Feature Description for Recommending Feature Selection Algorithm}},
	author       = {Filchenkov, Andrey and Pendryak, Arseniy},
	year         = 2015,
	booktitle    = {2015 Artificial Intelligence and Natural Language and Information Extraction, Social Media and Web Search FRUCT Conference (AINL-ISMW FRUCT)},
	volume       = {},
	number       = {},
	pages        = {11--18},
	isbn = {978-9-5268-3970-7},
	doi          = {10.1109/AINL-ISMW-FRUCT.2015.7382962}
}
@inproceedings{lindner1999ast,
  title = {AST: Support for Algorithm Selection with a CBR Approach},
  author = {Guido Lindner and Rudi Studer},
  year = {1999},
  tags = {systematic-approach},
  researchr = {https://researchr.org/publication/LindnerS99},
  cites = {0},
  citedby = {0},
  pages = {418-423},
  booktitle = {Principles of Data Mining and Knowledge Discovery, Third European Conference, PKDD  99, Prague, Czech Republic, September 15-18, 1999, Proceedings},
  editor = {Jan M. Zytkow and Jan Rauch},
  volume = {1704},
  series = {Lecture Notes in Computer Science},
  publisher = {Springer},
  isbn = {3-540-66490-4},
	doi             = {10.1007/978-3-540-48247-5_52},
}
@book{michie1994machine,
	author = {Michie, D. and Spiegelhalter, D. and Taylor, Charles},
	year = {1999},
	month = {01},
	pages = {},
	title = {{Machine Learning, Neural and Statistical Classification}},
	volume = {37},
	journal = {Technometrics},
	isbn = {978-0-13-106360-0},
	doi = {10.2307/1269742}
}
@phdthesis{vanschoren2010understanding,
  author    = {Joaquin Vanschoren},
  title     = {{Understanding Machine Learning Performance with Experiment Databases}},
  school    = {Katholieke Universiteit Leuven, Belgium},
  year      = {2010},
  url       = {https://lirias.kuleuven.be/handle/123456789/266060},
  timestamp = {Thu, 12 Mar 2020 11:23:59 +0100},
  biburl    = {https://dblp.org/rec/phd/basesearch/Vanschoren10.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{segrera2008information,
	abstract = {Information-theoretic measures are suitable to characterize datasets with discrete attributes (or continuous which can be transformed). They can find information that can be decisive in order to analyze the behavior of different learning algorithms with specific datasets. The objective of the work presented in this paper is to study by means of three similar datasets from UCI Repository Machine Learning, the possible reasons for which breast-cancer-wisconsin dataset, in comparison with other 20 datasets, showed in a previous research that Stacking by Meta-Decision Trees (MDT) was significant better than all other multiclassifier models, including Stacking by Multi-Response Linear Regression (MLR). In our experiments the proportion of missing values, among other significant changes in different measure values, provided evidences about the possible origin of the different behaviors presented by these multiclassifier schemes depending on data characteristics.},
	address = {Berlin, Heidelberg},
	author = {Segrera, Saddys and Pinho, Joel and Moreno, Mar{\'\i}a N.},
	booktitle = {Hybrid Artificial Intelligence Systems},
	editor = {Corchado, Emilio and Abraham, Ajith and Pedrycz, Witold},
	isbn = {978-3-540-87656-4},
	pages = {458--465},
	doi             = {https://doi.org/10.1007/978-3-540-87656-4_57},
	publisher = {Springer Berlin Heidelberg},
	title = {{Information-Theoretic Measures for Meta-learning}},
	year = {2008}}

@article{ho2002complexity,
  author={Tin Kam Ho and Basu, M.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={{Complexity Measures of Supervised Classification Problems}}, 
  year={2002},
  volume={24},
  number={3},
  pages={289-300},
  doi={10.1109/34.990132}}

@article{lorena2019complex,
author = {Lorena, Ana C. and Garcia, Lu\'{\i}s P. F. and Lehmann, Jens and Souto, Marcilio C. P. and Ho, Tin Kam},
title = {{How Complex Is Your Classification Problem? {A} Survey on Measuring Classification Complexity}},
year = {2019},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3347711},
doi = {10.1145/3347711},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {107},
numpages = {34},
keywords = {Supervised machine learning, complexity measures, classification}
}
@inproceedings{kopf2002combination,
	title        = {{Combination of Task Description Strategies and Case Base Properties for Meta-Learning}},
	author       = {K{\"o}pf, Christian and Iglezakis, Ioannis},
	year         = 2002,
	booktitle    = {Proceedings of the 2nd international workshop on integration and collaboration aspects of data mining, decision support and meta-learning},
	pages        = {65--76}
}

@inproceedings{perez1996learning,
  author={Eduardo Perez and Larry A. Rendell},
author = {P\'{e}rez, Eduardo and Rendell, Larry A.},
title = {{Learning despite Concept Variation by Finding Structure in Attribute-Based Data}},
year = {1996},
isbn = {1558604197},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the Thirteenth International Conference on International Conference on Machine Learning},
pages = {391–399},
numpages = {9},
location = {Bari, Italy},
series = {ICML'96}, 
doi             = {10.5555/3091696.3091743},
}

@article{munoz2018instance,
	abstract = {This paper tackles the issue of objective performance evaluation of machine learning classifiers, and the impact of the choice of test instances. Given that statistical properties or features of a dataset affect the difficulty of an instance for particular classification algorithms, we examine the diversity and quality of the UCI repository of test instances used by most machine learning researchers. We show how an instance space can be visualized, with each classification dataset represented as a point in the space. The instance space is constructed to reveal pockets of hard and easy instances, and enables the strengths and weaknesses of individual classifiers to be identified. Finally, we propose a methodology to generate new test instances with the aim of enriching the diversity of the instance space, enabling potentially greater insights than can be afforded by the current UCI repository.},
	author = {Mu{\~n}oz, Mario A. and Villanova, Laura and Baatar, Davaatseren and Smith-Miles, Kate},
	da = {2018/01/01},
	date-added = {2022-07-03 12:29:39 +0100},
	date-modified = {2022-07-03 12:29:39 +0100},
	doi = {10.1007/s10994-017-5629-5},
	id = {Mu{\~n}oz2018},
	isbn = {1573-0565},
	journal = {Machine Learning},
	number = {1},
	pages = {109--147},
	title = {{Instance Spaces for Machine Learning Classification}},
	ty = {JOUR},
	url = {https://doi.org/10.1007/s10994-017-5629-5},
	volume = {107},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1007/s10994-017-5629-5}}

@inproceedings{reif2011prediction,
	abstract = {Besides the classification performance, the training time is a second important factor that affects the suitability of a classification algorithm regarding an unknown dataset. An algorithm with a slightly lower accuracy is maybe preferred if its training time is significantly lower. Additionally, an estimation of the required training time of a pattern recognition task is very useful if the result has to be available in a certain amount of time.},
	address = {Berlin, Heidelberg},
	author = {Reif, Matthias and Shafait, Faisal and Dengel, Andreas},
	booktitle = {KI 2011: Advances in Artificial Intelligence},
	editor = {Bach, Joscha and Edelkamp, Stefan},
	isbn = {978-3-642-24455-1},
	pages = {260--271},
	publisher = {Springer Berlin Heidelberg},
	title = {{Prediction of Classifier Training Time Including Parameter Optimization}},
	year = {2011},
	doi             = {https://doi.org/10.1007/978-3-642-24455-1_25

},}

@article{wang2015improved,
	abstract = {Picking up appropriate classification algorithms for a given data set is very important and useful in practice. One of the most challenging issues for algorithm selection is how to characterize different data sets. Recently, we extracted the structural information of a data set to characterize itself. Although these kinds of characteristics work well in identifying similar data sets and recommending appropriate classification algorithms, the extraction method can only be applied to binary data sets and its performance is not high. Thus, in this paper, an improved data set characterization method is proposed to address these problems. For the purpose of evaluating the effectiveness of the improved method on algorithm recommendation, the unsupervised learning method EM is employed to build the algorithm recommendation model. Extensive experiments with 17 different types of classification algorithms are conducted upon 84 public UCI data sets; the results demonstrate the effectiveness of the proposed method.},
	author = {Wang, Guangtao and Song, Qinbao and Zhu, Xiaoyan},
	da = {2015/12/01},
	date-added = {2022-07-03 14:20:18 +0100},
	date-modified = {2022-07-03 14:20:18 +0100},
	doi = {10.1007/s10489-015-0689-3},
	id = {Wang2015},
	isbn = {1573-7497},
	journal = {Applied Intelligence},
	number = {4},
	pages = {892--912},
	title = {{An Improved Data Characterization Method and Its Application in Classification Algorithm Recommendation}},
	ty = {JOUR},
	url = {https://doi.org/10.1007/s10489-015-0689-3},
	volume = {43},
	year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1007/s10489-015-0689-3}}


@book{hebb1949organisation,
	title        = {{The Organisation of Behaviour: a Neuropsychological Theory}},
	author       = {Hebb, Donald Olding},
	year         = 1949,
	publisher    = {Science Editions New York},
	doi            = {10.4324/9781410612403},
	isbn           = {978-0805843002},
}
@incollection{cohen2021evolution,
	author = {Stanley Cohen},
	booktitle = {Artificial Intelligence and Deep Learning in Pathology},
	doi = {https://doi.org/10.1016/B978-0-323-67538-3.00001-4},
	editor = {Stanley Cohen},
	isbn = {978-0-323-67538-3},
	keywords = {Capsule, Core memory, Graphical, Instruction set, Neural networks, Neuromorphic computing, Probability, Statistics support vectors},
	pages = {1-12},
	publisher = {Elsevier},
	title = {{Chapter 1 - The Evolution of Machine Learning: Past, Present, and Future}},
	url = {https://www.sciencedirect.com/science/article/pii/B9780323675383000014},
	year = {2021},
	Bdsk-Url-1 = {https://www.sciencedirect.com/science/article/pii/B9780323675383000014},
	Bdsk-Url-2 = {https://doi.org/10.1016/B978-0-323-67538-3.00001-4}}

@article{snijders2012big,
  title={{"Big Data" : Big Gaps of Knowledge in the Field of Internet Science}},
  year={2012},
  number={1},
  volume={7},
  journal={International Journal of Internet Science},
  pages={1--5},
	issn            = {1662-5544},
  author={Snijders, Chris and Matzat, Uwe and Reips, Ulf-Dietrich}
}
@article{renear2010definitions,
	abstract = {Abstract The integration of heterogeneous data in varying formats and from diverse communities requires an improved understanding of the concept of a dataset, and of key related concepts, such as format, encoding, and version. Ultimately, a normative formal framework of such concepts will be needed to support the effective curation, integration, and use of shared multi-disciplinary scientific data. To prepare for the development of this framework we reviewed the definitions of dataset found in technical documentation and the scientific literature. Four basic features can be identified as common to most definitions: grouping, content, relatedness, and purpose. In this summary of our results we describe each of these features, indicating the directions a more formal analysis might take.},
	author = {Renear, Allen H. and Sacchi, Simone and Wickett, Karen M.},
	doi = {https://doi.org/10.1002/meet.14504701240},
	eprint = {https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/meet.14504701240},
	journal = {Proceedings of the American Society for Information Science and Technology},
	number = {1},
	pages = {1-4},
	title = {{Definitions of Dataset in the Scientific and Technical Literature}},
	url = {https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/meet.14504701240},
	volume = {47},
	year = {2010},
	Bdsk-Url-1 = {https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/meet.14504701240},
	Bdsk-Url-2 = {https://doi.org/10.1002/meet.14504701240}}

@inproceedings{toutanova2016dataset,
    title = "{A Dataset and Evaluation Metrics for Abstractive Compression of Sentences and Short Paragraphs}",
    author = "Toutanova, Kristina  and
      Brockett, Chris  and
      Tran, Ke M.  and
      Amershi, Saleema",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1033",
    doi = "10.18653/v1/D16-1033",
    pages = "340--350",
  }

@inproceedings{rubin1986basic,
  title={{An Overview of Multiple Imputation}},
  author={Rubin, Donald B},
  booktitle={Proceedings of the survey research methods section of the American statistical association},
  pages={79--84},
  year={1988},
  organization={Citeseer}
}
@inproceedings{drechsler2010using,
	abstract = {Generating synthetic datasets is an innovative approach for data dissemination. Values at risk of disclosure or even the entire dataset are replaced with multiple draws from statistical models. The quality of the released data strongly depends on the ability of these models to capture important relationships found in the original data. Defining useful models for complex survey data can be difficult and cumbersome. One possible approach to reduce the modeling burden for data disseminating agencies is to rely on machine learning tools to reveal important relationships in the data.},
	address = {Berlin, Heidelberg},
	author = {Drechsler, J{\"o}rg},
	booktitle = {Privacy in Statistical Databases},
	editor = {Domingo-Ferrer, Josep and Magkos, Emmanouil},
	isbn = {978-3-642-15838-4},
	pages = {148--161},
	publisher = {Springer Berlin Heidelberg},
	title = {{Using Support Vector Machines for Generating Synthetic Datasets}},
	year = {2010},
	doi             = {10.1007/978-3-642-15838-4_14},}

@article{rubin1993statistical,
author={Rubin,Donald B.},
year={1993},
month={06},
title={{Discussion Statistical Disclosure Limitation}},
journal={Journal of Official Statistics},
volume={9},
number={2},
pages={461},
note={Copyright - Copyright Statistics Sweden (SCB) Jun 1993; Last updated - 2013-01-07},
keywords={Statistics},
isbn={0282423X},
language={English},
url={https://www.proquest.com/scholarly-journals/discussion-statistical-disclosure-limitation/docview/1266818482/se-2},
}
@inproceedings{dandekar2018comparative,
	doi             = {10.1007/978-3-319-98812-2_35},
	abstract = {Unrestricted availability of the datasets is important for the researchers to evaluate their strategies to solve the research problems. While publicly releasing the datasets, it is equally important to protect the privacy of the respective data owners. Synthetic datasets that preserve the utility while protecting the privacy of the data owners stands as a midway.},
	address = {Cham},
	author = {Dandekar, Ashish and Zen, Remmy A. M. and Bressan, St{\'e}phane},
	booktitle = {Database and Expert Systems Applications},
	editor = {Hartmann, Sven and Ma, Hui and Hameurlain, Abdelkader and Pernul, G{\"u}nther and Wagner, Roland R.},
	isbn = {978-3-319-98812-2},
	pages = {387--395},
	publisher = {Springer International Publishing},
	title = {{A Comparative Study of Synthetic Dataset Generation Techniques}},
	year = {2018}}

@article{haslett2009cloning,
	title        = {Cloning data: generating datasets with exactly the same multiple linear regression fit},
	author       = {Haslett, SJ and Govindaraju, K},
	year         = 2009,
	journal      = {Australian \& New Zealand Journal of Statistics},
	publisher    = {Wiley Online Library},
	volume       = 51,
	number       = 4,
	pages        = {499--503}
}
@article{kirchner2004decision,
	author = {K. Kirchner and K.-H. T{\"o}lle and J. Krieter},
	doi = {https://doi.org/10.1016/j.livprodsci.2004.04.003},
	issn = {0301-6226},
	journal = {Livestock Production Science},
	keywords = {Decision tree, Decision rules, C4.5-algorithm, Real swine breeding datasets, Culling strategy},
	number = {2},
	pages = {191-200},
	title = {{Decision Tree Technique Applied to Pig Farming Datasets}},
	url = {https://www.sciencedirect.com/science/article/pii/S0301622604001009},
	volume = {90},
	year = {2004},
	Bdsk-Url-1 = {https://www.sciencedirect.com/science/article/pii/S0301622604001009},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.livprodsci.2004.04.003}}

@article{reiter2005using,
	author={Reiter, Jerome P},
	year={2005},
	month={09},
	title={{Using CART to Generate Partially Synthetic Public Use Microdata}},
	journal={Journal of Official Statistics},
	volume={21},
	number={3},
	pages={441},
	note={Copyright - Copyright Statistics Sweden (SCB) Sep 2005; Last updated - 2013-01-07},
	abstract={To limit disclosure risks, one approach is to release partially synthetic public use microdata sets. These comprise the units originally surveyed, but some collected values, for example sensitive values at high risk of disclosure or values of key identifiers, are replaced with multiple imputations. This article presents and evaluates the use of classification and regression trees to generate partially synthetic data. Two potential applications of CART are studied via simulation: (i) generate synthetic data for sensitive variables; and, (ii) generate synthetic data for variables that are key identifiers.},
	keywords={Statistics; CART; confidentiality; disclosure; multiple imputation; synthetic data; trees},
	isbn={0282423X},
	language={English},
	url={https://www.proquest.com/scholarly-journals/using-cart-generate-partially-synthetic-public/docview/1266792149/se-2},
}
@article{song2015decision,
	title        = {{Decision Tree Methods: Applications for Classification and Prediction}},
	author       = {Song, Yan-Yan and Ying, LU},
	year         = 2015,
	journal      = {Shanghai archives of psychiatry},
	publisher    = {Shanghai Mental Health Center},
	volume       = 27,
	number       = 2,
	pages        = 130,
	doi             = {10.11919/j.issn.1002-0829.215044},
}
@article{caiola2010random,
author = {Caiola, Gregory and Reiter, Jerome P.},
title = {{Random Forests for Generating Partially Synthetic, Categorical Data}},
year = {2010},
issue_date = {April 2010},
publisher = {IIIA-CSIC},
address = {Bellaterra, Catalonia, ESP},
volume = {3},
number = {1},
issn = {1888-5063},
abstract = {Several national statistical agencies are now releasing partially synthetic, public use microdata. These comprise the units in the original database with sensitive or identifying values replaced with values simulated from statistical models. Specifying synthesis models can be daunting in databases that includemany variables of diverse types. These variablesmay be related inways that can be difficult to capture with standard parametric tools. In this article, we describe how random forests can be adapted to generate partially synthetic data for categorical variables. Using an empirical study, we illustrate that the random forest synthesizer can preserve relationships reasonably well while providing low disclosure risks. The random forest synthesizer has some appealing features for statistical agencies: it can be applied with minimal tuning, easily incorporates numerical, categorical, and mixed variables as predictors, operates efficiently in high dimensions, and automatically fits non-linear relationships.},
journal = {Transactions on Data Privacy},
month = {apr},
pages = {27–42},
numpages = {16},
doi             = {10.5555/1747335.1747337},
}
@article{breiman2001random,
	author = {Breiman, Leo},
	da = {2001/10/01},
	date-added = {2022-02-28 16:44:29 +0000},
	date-modified = {2022-02-28 16:44:29 +0000},
	doi = {10.1023/A:1010933404324},
	id = {Breiman2001},
	isbn = {1573-0565},
	journal = {Machine Learning},
	number = {1},
	pages = {5--32},
	title = {Random Forests},
	ty = {JOUR},
	url = {https://doi.org/10.1023/A:1010933404324},
	volume = {45},
	year = {2001},
	Bdsk-Url-1 = {https://doi.org/10.1023/A:1010933404324}
  }
@book{abdi1999neural,
	title        = {Neural networks},
	author       = {Abdi, Herv{\'e} and Valentin, Dominique and Edelman, Betty},
	year         = 1999,
	publisher    = {Sage Publications, Inc},
	doi             = {10.4135/9781412985277},
	series          = {Quantitative Applications in the Social Sciences},
	isbn = {9781412985277}
}
@book{summerfield2007rapid,
  title = {{Rapid GUI Programming with Python and Qt: The Definitive Guide to PyQt Programming (paperback)}},
  author = {Summerfield, Mark},
  year = {2007},
  publisher = {Pearson Education},
	isbn = {978-0134393339}
}
@article{karr2006framework,
	title        = {A framework for evaluating the utility of data altered to protect confidentiality},
	author       = {Karr, Alan F and Kohnen, Christine N and Oganian, Anna and Reiter, Jerome P and Sanil, Ashish P},
	year         = 2006,
	journal      = {The American Statistician},
	publisher    = {Taylor \& Francis},
	volume       = 60,
	number       = 3,
	pages        = {224--232}
}
@inproceedings{priyanka2016analysis,
  author={Priyanka, S Selva and Galgali, Sudeep and Priya, S Selva and Shashank, B R and Srinivasa, K G},
  booktitle={2016 International Conference on Circuits, Controls, Communications and Computing (I4C)}, 
  title={{Analysis of Suicide Victim Data for the Prediction of Number of Suicides in India}}, 
  year={2016},
  volume={},
  number={},
  pages={1-5},
  doi={10.1109/CIMCA.2016.8053293},
	isbn = {978-1-5090-5369-8}
}
@article{anantrasirichai2019deep,
	abstract = {Satellites enable widespread, regional or global surveillance of volcanoes and can provide the first indication of volcanic unrest or eruption. Here we consider Interferometric Synthetic Aperture Radar (InSAR), which can be employed to detect surface deformation with a strong statistical link to eruption. Recent developments in technology as well as improved computational power have resulted in unprecedented quantities of monitoring data, which can no longer be inspected manually. The ability of machine learning to automatically identify signals of interest in these large InSAR datasets has already been demonstrated, but data-driven techniques, such as convolutional neutral networks (CNN) require balanced training datasets of positive and negative signals to effectively differentiate between real deformation and noise. As only a small proportion of volcanoes are deforming and atmospheric noise is ubiquitous, the use of machine learning for detecting volcanic unrest is more challenging than many other applications. In this paper, we address this problem using synthetic interferograms to train the AlexNet CNN. The synthetic interferograms are composed of 3 parts: 1) deformation patterns based on a Monte Carlo selection of parameters for analytic forward models, 2) stratified atmospheric effects derived from weather models and 3) turbulent atmospheric effects based on statistical simulations of correlated noise. The AlexNet architecture trained with synthetic data outperforms that trained using real interferograms alone, based on classification accuracy and positive predictive value (PPV). However, the models used to generate the synthetic signals are a simplification of the natural processes, so we retrain the CNN with a combined dataset consisting of synthetic models and selected real examples, achieving a final PPV of 82%. Although applying atmospheric corrections to the entire dataset is computationally expensive, it is relatively simple to apply them to the small subset of positive results. This further improves the detection performance without a significant increase in computational burden (PPV of 100%). Thus, we demonstrate that training with synthetic examples can improve the ability of CNNs to detect volcano deformation in satellite images, and propose an efficient workflow for the development of automated systems.},
	author = {Anantrasirichai, Nantheera and Biggs, Juliet and Albino, Fabien and Bull, David},
	doi = {https://doi.org/10.1016/j.rse.2019.04.032},
	issn = {0034-4257},
	journal = {Remote Sensing of Environment},
	keywords = {Interferometric Synthetic Aperture Radar, Volcano, Machine learning, Detection},
	pages = {111179},
	title = {{A Deep Learning Approach to Detecting Volcano Deformation from Satellite Imagery Using Synthetic Datasets}},
	url = {https://www.sciencedirect.com/science/article/pii/S003442571930183X},
	volume = {230},
	year = {2019},
	Bdsk-Url-1 = {https://www.sciencedirect.com/science/article/pii/S003442571930183X},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.rse.2019.04.032}}

@inproceedings{yu2011geo,
  author={Yu, Xiao and Pan, Ang and Tang, Lu-An and Li, Zhenhui and Han, Jiawei},
  booktitle={2011 International Conference on Advances in Social Networks Analysis and Mining}, 
  title={{Geo-Friends Recommendation in GPS-based Cyber-physical Social Network}}, 
  year={2011},
  volume={},
  number={},
  pages={361-368},
  doi={10.1109/ASONAM.2011.118},
	isbn            = {978-1-61284-758-0}}
@inproceedings{sethi2017sms,
  author={Sethi, Paras and Bhandari, Vaibhav and Kohli, Bhavna},
  booktitle={2017 International Conference on Computing and Communication Technologies for Smart Nation (IC3TSN)}, 
  title={{SMS Spam Detection and Comparison of Various Machine Learning Algorithms}}, 
  year={2017},
  volume={},
  number={},
  pages={28-31},
	isbn            = {978-1-5386-0627-8},
  doi={10.1109/IC3TSN.2017.8284445}}

@INPROCEEDINGS{hamsapriya2011spam,
  author={Renuka, D. Karthika and Hamsapriya, T. and Chakkaravarthi, M. Raja and Surya, P. Lakshmi},
  booktitle={2011 International Conference on Process Automation, Control and Computing}, 
  title={{Spam Classification Based on Supervised Learning Using Machine Learning Techniques}}, 
  year={2011},
  volume={},
  number={},
  pages={1-7},
  doi={10.1109/PACC.2011.5979035}}
@inproceedings{kumar2018comparative,
	author={Kumar, Indu and Dogra, Kiran and Utreja, Chetna and Yadav, Premlata},
  booktitle={2018 Second International Conference on Inventive Communication and Computational Technologies (ICICCT)}, 
  title={{A Comparative Study of Supervised Machine Learning Algorithms for Stock Market Trend Prediction}}, 
  year={2018},
  volume={},
  number={},
  pages={1003-1007},
  doi={10.1109/ICICCT.2018.8473214}
}
@article{bilalli2017predictive,
	author = {Bilalli, Besim and Abell\'{o}, Alberto and Aluja-Banet, Tom\'{\i}s},
	title = {{On the Predictive Power of Meta-Features in OpenML}},
	year = {2017},
	issue_date = {20 12 2017},
	publisher = {Walter de Gruyter & Co.},
	address = {USA},
	volume = {27},
	number = {4},
	issn = {1641-876X},
	url = {https://doi.org/10.1515/amcs-2017-0048},
	doi = {10.1515/amcs-2017-0048},
	abstract = {Abstract The demand for performing data analysis is steadily rising. As a consequence, people of different profiles i.e., nonexperienced users have started to analyze their data. However, this is challenging for them. A key step that poses difficulties and determines the success of the analysis is data mining model/algorithm selection problem. Meta-learning is a technique used for assisting non-expert users in this step. The effectiveness of meta-learning is, however, largely dependent on the description/characterization of datasets i.e., meta-features used for meta-learning. There is a need for improving the effectiveness of meta-learning by identifying and designing more predictive meta-features. In this work, we use a method from exploratory factor analysis to study the predictive power of different meta-features collected in OpenML, which is a collaborative machine learning platform that is designed to store and organize meta-data about datasets, data mining algorithms, models and their evaluations. We first use the method to extract latent features, which are abstract concepts that group together meta-features with common characteristics. Then, we study and visualize the relationship of the latent features with three different performance measures of four classification algorithms on hundreds of datasets available in OpenML, and we select the latent features with the highest predictive power. Finally, we use the selected latent features to perform meta-learning and we show that our method improves the meta-learning process. Furthermore, we design an easy to use application for retrieving different meta-data from OpenML as the biggest source of data in this domain.},
	journal = {International Journal of Applied Mathematics and Computer Science},
	month = {dec},
	pages = {697–712},
	numpages = {16},
	keywords = {feature selection, feature extraction, meta-learning}
}
@article{rivolli2022meta,
	abstract = {Meta-learning is increasingly used to support the recommendation of machine learning algorithms and their configurations. These recommendations are made based on meta-data, consisting of performance evaluations of algorithms and characterizations on prior datasets. These characterizations, also called meta-features, describe properties of the data which are predictive for the performance of machine learning algorithms trained on them. Unfortunately, despite being used in many studies, meta-features are not uniformly described, organized and computed, making many empirical studies irreproducible and hard to compare. This paper aims to deal with this by systematizing and standardizing data characterization measures for classification datasets used in meta-learning. Moreover, it presents an extensive list of meta-features and characterization tools, which can be used as a guide for new practitioners. By identifying particularities and subtle issues related to the characterization measures, this survey points out possible future directions that the development of meta-features for meta-learning can assume.},
	author = {Adriano Rivolli and Lu{\'\i}s P.F. Garcia and Carlos Soares and Joaquin Vanschoren and Andr{\'e} C.P.L.F. {de Carvalho}},
	doi = {https://doi.org/10.1016/j.knosys.2021.108101},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Meta-features, Characterization measures, Meta-learning, Classification problems},
	pages = {108101},
	title = {{Meta-Features for Meta-Learning}},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121011631},
	volume = {240},
	year = {2022},
	Bdsk-Url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121011631},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.knosys.2021.108101}}

@book{weisberg1992central,
	title        = {{Central Tendency and Variability}},
	author       = {Weisberg, Herbert and Weisberg, Herbert F},
	  number={83},
	year         = 1992,
	publisher    = {Sage},
}
@article{mclachlan2005discriminant,
	author = {William S. Rayens},
	doi = {10.1080/00401706.1993.10485331},
	eprint = {https://www.tandfonline.com/doi/pdf/10.1080/00401706.1993.10485331},
	journal = {Technometrics},
	number = {3},
	pages = {324-326},
	publisher = {Taylor & Francis},
	title = {{Discriminant Analysis and Statistical Pattern Recognition}},
	url = {https://www.tandfonline.com/doi/abs/10.1080/00401706.1993.10485331},
	volume = {35},
	year = {1993},
	Bdsk-Url-1 = {https://www.tandfonline.com/doi/abs/10.1080/00401706.1993.10485331},
	Bdsk-Url-2 = {https://doi.org/10.1080/00401706.1993.10485331},
	isbn           = {978-0-471-69115-0},
	}

@incollection{kramer2013k,
	abstract = {This chapter gives an introduction to pattern recognition and machine learning via K-nearest neighbors. Nearest neighbor methods will have an important part to play in this book. The chapter starts with an introduction to foundations in machine learning and decision theory with a focus on classification and regression. For the model selection problem, basic methods like cross-validation are introduced. Nearest neighbor methods are based on the labels of the K-nearest patterns in data space. As local methods, nearest neighbor techniques are known to be strong in case of large data sets and low dimensions. Variants for multi-label classification, regression, and semi supervised learning settings allow the application to a broad spectrum of machine learning problems. Decision theory gives valuable insights into the characteristics of nearest neighbor learning results.},
	address = {Berlin, Heidelberg},
	author = {Kramer, Oliver},
	booktitle = {Dimensionality Reduction with Unsupervised Nearest Neighbors},
	doi = {10.1007/978-3-642-38652-7_2},
	isbn = {978-3-642-38652-7},
	pages = {13--23},
	publisher = {Springer Berlin Heidelberg},
	title = {{K-Nearest Neighbors}},
	url = {https://doi.org/10.1007/978-3-642-38652-7_2},
	year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-3-642-38652-7_2}}

@inproceedings{ler2018algorithm,
  author={Ler, Daren and Teng, Hongyu and He, Yu and Gidijala, Rahul},
  booktitle={2018 IEEE International Conference on Big Data (Big Data)}, 
  title={{Algorithm Selection for Classification Problems via Cluster-based Meta-features}}, 
  year={2018},
  volume={},
  number={},
  pages={4952-4960},
	isbn            = {978-1-5386-5035-6},
  doi={10.1109/BigData.2018.8621982}}
@inproceedings{boggs2014synthetic,
	abstract = {Measuring security controls across multiple layers of defense requires realistic data sets and repeatable experiments. However, data sets that are collected from real users often cannot be freely exchanged due to privacy and regulatory concerns. Synthetic datasets, which can be shared, have in the past had critical flaws or at best been one time collections of data focusing on a single layer or type of data. We present a framework for generating synthetic datasets with normal and attack data for web applications across multiple layers simultaneously. The framework is modular and designed for data to be easily recreated in order to vary parameters and allow for inline testing. We build a prototype data generator using the framework to generate nine datasets with data logged on four layers: network, file accesses, system calls, and database simultaneously. We then test nineteen security controls spanning all four layers to determine their sensitivity to dataset changes, compare performance even across layers, compare synthetic data to real production data, and calculate combined defense in depth performance of sets of controls.},
	address = {Cham},
	author = {Boggs, Nathaniel and Zhao, Hang and Du, Senyao and Stolfo, Salvatore J.},
	booktitle = {Research in Attacks, Intrusions and Defenses},
	editor = {Stavrou, Angelos and Bos, Herbert and Portokalidis, Georgios},
	isbn = {978-3-319-11379-1},
	pages = {234--254},
	publisher = {Springer International Publishing},
	title = {{Synthetic Data Generation and Defense in Depth Measurement of Web Applications}},
	year = {2014},
	doi             = {10.1007/978-3-319-11379-1_12},}

@inproceedings{campagna2019genie,
	author = {Campagna, Giovanni and Xu, Silei and Moradshahi, Mehrad and Socher, Richard and Lam, Monica S.},
	title = {{Genie: A Generator of Natural Language Semantic Parsers for Virtual Assistant Commands}},
	year = {2019},
	isbn = {9781450367127},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3314221.3314594},
	doi = {10.1145/3314221.3314594},
	abstract = {To understand diverse natural language commands, virtual assistants today are trained with numerous labor-intensive, manually annotated sentences. This paper presents a methodology and the Genie toolkit that can handle new compound commands with significantly less manual effort. We advocate formalizing the capability of virtual assistants with a Virtual Assistant Programming Language (VAPL) and using a neural semantic parser to translate natural language into VAPL code. Genie needs only a small realistic set of input sentences for validating the neural model. Developers write templates to synthesize data; Genie uses crowdsourced paraphrases and data augmentation, along with the synthesized data, to train a semantic parser. We also propose design principles that make VAPL languages amenable to natural language translation. We apply these principles to revise ThingTalk, the language used by the Almond virtual assistant. We use Genie to build the first semantic parser that can support compound virtual assistants commands with unquoted free-form parameters. Genie achieves a 62% accuracy on realistic user inputs. We demonstrate Genie’s generality by showing a 19% and 31% improvement over the previous state of the art on a music skill, aggregate functions, and access control.},
	booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
	pages = {394–410},
	numpages = {17},
	keywords = {training data generation, data augmentation, semantic parsing, virtual assistants, data engineering},
	location = {Phoenix, AZ, USA},
	series = {Programming Language Design and Implementation 2019}
}
@inproceedings{kollar2018alexa,
	title = "{The {A}lexa Meaning Representation Language}",
    author = "Kollar, Thomas  and
      Berry, Danielle  and
      Stuart, Lauren  and
      Owczarzak, Karolina  and
      Chung, Tagyoung  and
      Mathias, Lambert  and
      Kayser, Michael  and
      Snow, Bradford  and
      Matsoukas, Spyros",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans - Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-3022",
    doi = "10.18653/v1/N18-3022",
    pages = "177--184",
    abstract = "This paper introduces a meaning representation for spoken language understanding. The Alexa meaning representation language (AMRL), unlike previous approaches, which factor spoken utterances into domains, provides a common representation for how people communicate in spoken language. AMRL is a rooted graph, links to a large-scale ontology, supports cross-domain queries, fine-grained types, complex utterances and composition. A spoken language dataset has been collected for Alexa, which contains ∼20k examples across eight domains. A version of this meaning representation was released to developers at a trade show in 2016.",
}
@inproceedings{key,
  author          = {},
  booktitle       = {},
  editor          = {},
  title           = {},
  year            = {}
}
@article{dunson2009nonparametric,
	author = {David B. Dunson and Chuanhua Xing},
	doi = {10.1198/jasa.2009.tm08439},
	eprint = {https://doi.org/10.1198/jasa.2009.tm08439},
	journal = {Journal of the American Statistical Association},
	number = {487},
	pages = {1042-1051},
	publisher = {Taylor & Francis},
	title = {{Nonparametric Bayes Modeling of Multivariate Categorical Data}},
	url = {https://doi.org/10.1198/jasa.2009.tm08439},
	volume = {104},
	year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1198/jasa.2009.tm08439}}

@book{trumpler1953statistical,
	title        = {Statistical astronomy},
	author       = {Trumpler, Robert Julius and Weaver, Harold F},
	year         = 1953,
	publisher    = {Dover Books on Astronomy and Space Topics}
}
@misc{rennie2005mixtures,
	title        = {{Mixtures of Multinomials}},
	author       = {Rennie, Jason DM},
	year         = 2005,
	url            = {http://people.csail.mit.edu/jrennie/writing/mixtureMultinomials.pdf},
}
@article{dash1997feature,
	title        = {Feature selection for classification},
	author       = {Dash, Manoranjan and Liu, Huan},
	year         = 1997,
	journal      = {Intelligent data analysis},
	publisher    = {Elsevier},
	volume       = 1,
	number       = {1-4},
	pages        = {131--156}
}
@inproceedings{4295518,
	title        = {Distance Measure Assisted Rough Set Feature Selection},
	author       = {Parthalain, Neil Mac and Shen, Qiang and Jensen, Richard},
	year         = 2007,
	booktitle    = {2007 IEEE International Fuzzy Systems Conference},
	volume       = {},
	number       = {},
	pages        = {1--6},
	doi          = {10.1109/FUZZY.2007.4295518}
}
@inproceedings{birsan2005one,
	title        = {One hundred years since the introduction of the set distance by Dimitrie Pompeiu},
	author       = {Birsan, T and Tiba, Dan},
	year         = 2005,
	booktitle    = {IFIP Conference on System Modeling and Optimization},
	pages        = {35--39},
	organization = {Springer}
}
@article{oh2011new,
	title        = {A new dataset evaluation method based on category overlap},
	author       = {Oh, Sejong},
	year         = 2011,
	journal      = {Computers in Biology and Medicine},
	publisher    = {Elsevier},
	volume       = 41,
	number       = 2,
	pages        = {115--122}
}
@misc{Mirza,
	doi = {10.48550/ARXIV.1411.1784}, 
  url = {https://arxiv.org/abs/1411.1784},
  author = {Mirza, Mehdi and Osindero, Simon},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {{Conditional Generative Adversarial Nets}},
  publisher = {arXiv},
  year = {2014},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{gan,
  doi = {0.5555/3295222.3295327},
author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron},
title = {Improved Training of Wasserstein GANs},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only poor samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models with continuous generators. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {5769–5779},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}

}

@techreport{holbrook,
	title        = {{Intro to Deep Learning}},
	author       = {Holbrook R., Cook A},
	year         = 2020,
	institution  = {Kaggle}
}
@article{laberge2011advising,
	author = {Yves Laberge},
	doi = {10.1080/02664763.2011.559375},
	eprint = {https://doi.org/10.1080/02664763.2011.559375},
	journal = {Journal of Applied Statistics},
	number = {12},
	pages = {2991-2991},
	publisher = {Taylor & Francis},
	title = {{Advising on Research Methods: A Consultant's Companion}},
	url = {https://doi.org/10.1080/02664763.2011.559375},
	volume = {38},
	year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1080/02664763.2011.559375}}

@inproceedings{8226246,
	title        = {{Eager Decision Tree}},
	author       = {Gavankar, Sachin S. and Sawarkar, Sudhirkumar D.},
	year         = 2017,
	booktitle    = {2017 2nd International Conference for Convergence in Technology (I2CT)},
	volume       = {},
	number       = {},
	pages        = {837--840},
	isbn            = {978-1-5090-4307-1},
	doi          = {10.1109/I2CT.2017.8226246}
}
@article{mckinney2011pandas,
  title={{Pandas: A Foundational Python Library for Data Analysis and Statistics}},
  author={McKinney, Wes and others},
  journal={Python for high performance and scientific computing},
  volume={14},
  number={9},
  pages={1--9},
  year={2011},
  publisher={Seattle}
}
@article{alcobacca2020mfe,
  author    = {Edesio Alcoba{\c{c}}a and
               Felipe Siqueira and
               Adriano Rivolli and
               Lu{\'{\i}}s Paulo F. Garcia and
               Jefferson Tales Oliva and
               Andr{\'{e}} C. P. L. F. de Carvalho},
  title     = {{MFE: Towards Reproducible Meta-Feature Extraction}},
  journal   = {Journal of Machine Learning Research},
  volume    = {21},
  pages     = {111:1--111:5},
  year      = {2020},
  url       = {http://jmlr.org/papers/v21/19-348.html},
  timestamp = {Wed, 18 Nov 2020 15:58:12 +0100},
  biburl    = {https://dblp.org/rec/journals/jmlr/AlcobacaSRGOC20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{ziegel2003elements,
	author = {Eric R Ziegel},
	doi = {10.1198/tech.2003.s770},
	eprint = {https://doi.org/10.1198/tech.2003.s770},
	journal = {Technometrics},
	number = {3},
	pages = {267-268},
	publisher = {Taylor & Francis},
	title = {{The Elements of Statistical Learning}},
	url = {https://doi.org/10.1198/tech.2003.s770},
	volume = {45},
	year = {2003},
	Bdsk-Url-1 = {https://doi.org/10.1198/tech.2003.s770}}


@article{ray2021various,
	abstract = {In the era of healthcare, and its related research fields, the dimensionality problem of high dimensional data is a massive challenge as it contains a huge number of variables forming complex data matrices. The demand for dimension reduction of complex data is growing immensely to improvise data prediction, analysis and visualization. In general, dimension reduction techniques are defined as a compression of dataset from higher dimensional matrix to lower dimensional matrix. Several computational techniques have been implemented for data dimension reduction, which is further segregated into two categories such as feature extraction and feature selection. In this review, a detailed investigation of various feature extraction and feature selection methods has been carried out with a systematic comparison of several dimension reduction techniques for the analysis of high dimensional data and to overcome the problem of data loss. Then, some case studies are also cited to verify the better approach for data dimension reduction by considering few advances described in the technical literature. This review paper may guide researchers to choose the most effective method for satisfactory analysis of high dimensional data.},
	author = {Ray, Papia and Reddy, S. Surender and Banerjee, Tuhina},
	da = {2021/06/01},
	date-added = {2022-07-03 12:48:01 +0100},
	date-modified = {2022-07-03 12:48:01 +0100},
	doi = {10.1007/s10462-020-09928-0},
	id = {Ray2021},
	isbn = {1573-7462},
	journal = {Artificial Intelligence Review},
	number = {5},
	pages = {3473--3515},
	title = {{Various Dimension Reduction Techniques for High Dimensional Data Analysis: A Review}},
	ty = {JOUR},
	url = {https://doi.org/10.1007/s10462-020-09928-0},
	volume = {54},
	year = {2021},
	Bdsk-Url-1 = {https://doi.org/10.1007/s10462-020-09928-0}}



@article{doi:10.1080/10691898.2011.11889611,
	author = {David P. Doane and Lori E. Seward},
	doi = {10.1080/10691898.2011.11889611},
	eprint = {https://doi.org/10.1080/10691898.2011.11889611},
	journal = {Journal of Statistics Education},
	number = {2},
	pages = {null},
	publisher = {Taylor & Francis},
	title = {{Measuring Skewness: A Forgotten Statistic?}},
	url = {https://doi.org/10.1080/10691898.2011.11889611},
	volume = {19},
	year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1080/10691898.2011.11889611}}

@article{dean2018descriptive,
  title={{Descriptive Statistics: Skewness and the Mean, Median, and Mode}},
  author={Dean, Susan and Illowsky, Barbara},
  journal={Connexions website},
  year={2018},
	url             = {https://resources.saylor.org/wwwresources/archived/site/wp-content/uploads/2011/06/MA121-1.3.3.pdf},
}

@article{doi:10.1080/00031305.1988.10475539,
	author = {Kevin P. Balanda and H. L. Macgillivray},
	doi = {10.1080/00031305.1988.10475539},
	eprint = {https://www.tandfonline.com/doi/pdf/10.1080/00031305.1988.10475539},
	journal = {The American Statistician},
	number = {2},
	pages = {111-119},
	publisher = {Taylor & Francis},
	title = {{Kurtosis: A Critical Review}},
	url = {https://www.tandfonline.com/doi/abs/10.1080/00031305.1988.10475539},
	volume = {42},
	year = {1988},
	Bdsk-Url-1 = {https://www.tandfonline.com/doi/abs/10.1080/00031305.1988.10475539},
	Bdsk-Url-2 = {https://doi.org/10.1080/00031305.1988.10475539}}


@article{doi:10.1080/00031305.1970.10478885,
	author = {Richard B. Darlington},
	doi = {10.1080/00031305.1970.10478885},
	eprint = {https://doi.org/10.1080/00031305.1970.10478885},
	journal = {The American Statistician},
	number = {2},
	pages = {19-22},
	publisher = {Taylor & Francis},
	title = {{Is Kurtosis Really ``Peakedness?''}},
	url = {https://doi.org/10.1080/00031305.1970.10478885},
	volume = {24},
	year = {1970},
	Bdsk-Url-1 = {https://doi.org/10.1080/00031305.1970.10478885}}

@article{navarro2019learning,
  title={{Learning Statistics With Jamovi: A Tutorial for Psychology Students and Other Beginners (Version 0.70)}},
  author={Navarro, Danielle and Foxcroft, David},
  journal={Tillg{\"a}nglig online: \url{http://learnstatswithjamovi.com} [H{\"a}mtad 14 december]},
  year={2019},
	doi             = {10.24384/hgc3-7p15},
}

@article{ramya2017breast,
  title={{Breast Cancer Detection and Classification Using Ultrasound and Ultrasound Elastography Images}},
  author={Ramya, S and Nanda, S},
  journal={International Research Journal of Engineering and Technology},
  volume={4},
  pages={596--601},
  year={2017},
	issn            = {2395-0056},
}

@ARTICLE{5725235,
  author={Millman, K. Jarrod and Aivazis, Michael},
  journal={Computing in Science \& Engineering}, 
  title={{Python for Scientists and Engineers}}, 
  year={2011},
  volume={13},
  number={2},
  pages={9-12},
  doi={10.1109/MCSE.2011.36}}

@book{siahaan2019postgresql,
  title={POSTGRESQL FOR PYTHON GUI: A Progressive Tutorial to Develop Database Project},
  author={Siahaan, Vivian and Sianipar, Rismon Hasiholan},
  year={2019},
  publisher={SPARTA PUBLISHING},
	isbn           = { 978-1686474712},
}

@article{akhter2019cyber,
  title={{Cyber Bullying Detection and Classification using Multinomial Na{\"\i}ve Bayes and Fuzzy Logic}},
  author={Akhter, Arnisha and Acharjee, Uzzal K and Polash, Md Masbaul A},
  year={2019},
	journal={Modern Education and Computer Science},
	doi             = {10.5815/ijmsc.2019.04.01},
}

@inproceedings{wong2021use,
  title={{The use of Big Data in Machine Learning Algorithm}},
  author={Wong, Yew Kee},
  booktitle={CS \& IT Conference Proceedings},
  volume={11},
  year={2021},
  organization={CS \& IT Conference Proceedings},
	url             = {https://csitcp.org/paper/11/1119csit11.pdf},
	doi             = {10.5121/csit.2021.111911}

}

@inproceedings{lateh2017handling,
  title={{Handling a Small Dataset Problem in Prediction Model by Employ a Artificial Data Generation Approach: A Review}},
  author={Lateh, Masitah Abdul and Muda, Azah Kamilah and Yusof, Zeratul Izzah Mohd and Muda, Noor Azilah and Azmi, Mohd Sanusi},
  booktitle={Journal of Physics: Conference Series},
  volume={892},
  number={1},
  pages={012016},
  year={2017},
  organization={IOP Publishing},
	doi             = {10.1088/1742-6596/892/1/012016},
}

@INPROCEEDINGS{9640223,
  author={Santos, Ricardo Dos and Aguilar, Jose and Puerto, Eduard},
  booktitle={2021 XLVII Latin American Computing Conference (CLEI)}, 
  title={{A Meta-Learning Architecture based on Linked Data}}, 
  year={2021},
  volume={},
  number={},
  pages={1-10},
	isbn            = {978-1-6654-9503-5},
  doi={10.1109/CLEI53233.2021.9640223}}


@article{KANDA2016393,
	abstract = {The Traveling Salesman Problem (TSP) is one of the most studied optimization problems. Various meta-heuristics (MHs) have been proposed and investigated on many instances of this problem. It is widely accepted that the best MH varies for different instances. Ideally, one should be able to recommend the best MHs for a new TSP instance without having to execute them. However, this is a very difficult task. We address this task by using a meta-learning approach based on label ranking algorithms. These algorithms build a mapping that relates the characteristics of those instances (i.e., the meta-features) with the relative performance (i.e., the ranking) of MHs, based on (meta-)data extracted from TSP instances that have been already solved by those MHs. The success of this approach depends on the quality of the meta-features that describe the instances. In this work, we investigate four different sets of meta-features based on different measurements of the properties of TSP instances: edge and vertex measures, complex network measures, properties from the MHs, and subsampling landmarkers properties. The models are investigated in four different TSP scenarios presenting symmetry and connection strength variations. The experimental results indicate that meta-learning models can accurately predict rankings of MHs for different TSP scenarios. Good solutions for the investigated TSP instances can be obtained from the prediction of rankings of MHs, regardless of the learning algorithm used at the meta-level. The experimental results also show that the definition of the set of meta-features has an important impact on the quality of the solutions obtained.},
	author = {Jorge Kanda and Andre de Carvalho and Eduardo Hruschka and Carlos Soares and Pavel Brazdil},
	doi = {https://doi.org/10.1016/j.neucom.2016.04.027},
	issn = {0925-2312},
	journal = {Neurocomputing},
	keywords = {Meta-learning, Meta-features, Label ranking, Meta-heuristics, Traveling Salesman Problem},
	pages = {393-406},
	title = {{Meta-learning to Select the Best Meta-Heuristic for the Traveling Salesman Problem: A Comparison of Meta-Features}},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231216302867},
	volume = {205},
	year = {2016},
	Bdsk-Url-1 = {https://www.sciencedirect.com/science/article/pii/S0925231216302867},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.neucom.2016.04.027}}


@article{disclosure,
	abstract = {Machine learning (ML) has made a significant impact in medicine and cancer research; however, its impact in these areas has been undeniably slower and more limited than in other application domains. A major reason for this has been the lack of availability of patient data to the broader ML research community, in large part due to patient privacy protection concerns. High-quality, realistic, synthetic datasets can be leveraged to accelerate methodological developments in medicine. By and large, medical data is high dimensional and often categorical. These characteristics pose multiple modeling challenges.},
	author = {Goncalves, Andre and Ray, Priyadip and Soper, Braden and Stevens, Jennifer and Coyle, Linda and Sales, Ana Paula},
	da = {2020/05/07},
	date-added = {2022-06-29 10:52:58 +0100},
	date-modified = {2022-06-29 10:52:58 +0100},
	doi = {10.1186/s12874-020-00977-1},
	id = {Goncalves2020},
	isbn = {1471-2288},
	journal = {BMC Medical Research Methodology},
	number = {1},
	pages = {108},
	title = {{Generation and Evaluation of Synthetic Patient Data}},
	ty = {JOUR},
	url = {https://doi.org/10.1186/s12874-020-00977-1},
	volume = {20},
	year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1186/s12874-020-00977-1}}

@INPROCEEDINGS{9074205,
  author={Jose, Navya and Chakravarthi, Bharathi Raja and Suryawanshi, Shardul and Sherly, Elizabeth and McCrae, John P.},
  booktitle={2020 6th International Conference on Advanced Computing and Communication Systems (ICACCS)}, 
  title={{A Survey of Current Datasets for Code-Switching Research}}, 
  year={2020},
  volume={},
  number={},
  pages={136-141},
	isbn            = {978-1-7281-5197-7},
  doi={10.1109/ICACCS48705.2020.9074205}}

@INPROCEEDINGS{6890935,
  author={Skopik, Florian and Settanni, Giuseppe and Fiedler, Roman and Friedberg, Ivo},
  booktitle={2014 Twelfth Annual International Conference on Privacy, Security and Trust}, 
  title={{Semi-Synthetic Data Set Generation for Security Software Evaluation}}, 
  year={2014},
  volume={},
  number={},
  pages={156-163},
	isbn            = {978-1-4799-3503-1},
  doi={10.1109/PST.2014.6890935}}


@article{cond,
	author = {Xingyu Zhou and Yuling Jiao and Jin Liu and Jian Huang},
	doi = {10.1080/01621459.2021.2016424},
	eprint = {https://doi.org/10.1080/01621459.2021.2016424},
	journal = {Journal of the American Statistical Association},
	number = {0},
	pages = {1-12},
	publisher = {Taylor & Francis},
	title = {A Deep Generative Approach to Conditional Sampling},
	url = {https://doi.org/10.1080/01621459.2021.2016424},
	volume = {0},
	year = {2022},
	Bdsk-Url-1 = {https://doi.org/10.1080/01621459.2021.2016424}}


@article{indmargins,
	author = {Perrakis, Konstantinos and Ntzoufras, Ioannis and Tsionas, Efthymios},
	year = {2013},
	month = {11},
	pages = {},
	title = {{On the Use of Marginal Posteriors in Marginal Likelihood Estimation Via Importance-Sampling}},
	volume = {77},
	journal = {Computational Statistics \& Data Analysis},
	doi = {10.1016/j.csda.2014.03.004}
}

@article{baye,
	author = {Serena H. Chen and Carmel A. Pollino},
	doi = {https://doi.org/10.1016/j.envsoft.2012.03.012},
	issn = {1364-8152},
	journal = {Environmental Modelling \& Software},
	keywords = {Good modelling practice, Bayesian belief network, Model evaluation, Bayes network, Ecological models, Integration},
	pages = {134-145},
	title = {{Good Practice in Bayesian Network Modelling}},
	url = {https://www.sciencedirect.com/science/article/pii/S1364815212001041},
	volume = {37},
	year = {2012},
	Bdsk-Url-1 = {https://www.sciencedirect.com/science/article/pii/S1364815212001041},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.envsoft.2012.03.012}
	}


@article{gans,
  author={Creswell, Antonia and White, Tom and Dumoulin, Vincent and Arulkumaran, Kai and Sengupta, Biswa and Bharath, Anil A.},
  journal={IEEE Signal Processing Magazine}, 
  title={{Generative Adversarial Networks: An Overview}}, 
  year={2018},
  volume={35},
  number={1},
  pages={53-65},
  doi={10.1109/MSP.2017.2765202}}


@book{regression,
	author = {Zocchi, SilvioSandoval and Manly, Bryan FJ},
	booktitle = {Proceedings of the 7th International Conference on Teaching Statistics},
	date = {2006},
	date-added = {2022-06-30 18:16:42 +0100},
	date-modified = {2022-06-30 18:16:42 +0100},
	title = {Generating Different Data Sets for Linear Regression Models With the Same Estimates},
	year = {2006}}


@article{over,
	author = {Roelofs, Rebecca and Fridovich-Keil, Sara and Miller, John and Shankar, Vaishaal and Hardt, Moritz and Recht, Benjamin and Schmidt, Ludwig},
	title = {{A Meta-Analysis of Overfitting in Machine Learning}},
	year = {2019},
	journal={Advances in Neural Information Processing Systems},
	volume          = {32},
	publisher = {Curran Associates Inc.},
	address = {Red Hook, NY, USA},
	abstract = {We conduct the first large meta-analysis of overfitting due to test set reuse in the machine learning community. Our analysis is based on over one hundred machine learning competitions hosted on the Kaggle platform over the course of several years. In each competition, numerous practitioners repeatedly evaluated their progress against a holdout set that forms the basis of a public ranking available throughout the competition. Performance on a separate test set used only once determined the final ranking. By systematically comparing the public ranking with the final ranking, we assess how much participants adapted to the holdout set over the course of a competition. Our study shows, somewhat surprisingly, little evidence of substantial overfitting. These findings speak to the robustness of the holdout method across different data domains, loss functions, model classes, and human analysts.},
	booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
	articleno = {823},
	numpages = {11}
}